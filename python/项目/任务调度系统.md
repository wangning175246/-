远程管理系统

salt ansible关注的是执行，关注的不是调度

调度，调度那哪些任务应该在哪些主机上执行。

#### 远程命令执行

​	有agent
​		自己实现agent 自己维护agent，最基本的功能就是可以执行命令，通信协议，怎么和master通信。
​		一般都是subprocess模块进行执行命令，尽量不要使用os.popen和os.system

​	无agent,使用已有的agent
​		sshd、snmp、powershell、winrm wmi

无agent使用的ssh执行命令:

​	paramiko库，通过这个库实现

​	fabric使用paramiko库实现的

#### subprocess模块的使用

 ```python
res=subprocess.Popen("dir",shell=True,stderr=subprocess.STDOUT,stdout=subprocess.PIPE)
 ```

第一个是要执行命令，可以是一个列表，可以是一个字符串，如果是字符串整个字符串当成一个命令解析，如果是列表，则第一个元素当作命令，其他元素当作参数。

* shell=True表示 前面的命令以bash -c “ ”的方式去执行前面的命令

  bash -l  -l表示以登陆的方式进行执行命令，以登陆的方式会加载limit文件和/etc/profile.~/.bash_profile

  cd 命令是shell的内置函数，所以需要使用shell的控制结果，内置函数的时候需要设置shell=True

  登陆的时候加载的配置文件，

  agent 通常以deamon的方式存在，agent启动之后修改环境变量对agent式不可见的，所以我们需要以-l的选项执行，以登陆shell的方式去执行，会加载最新的环境变量，

  limits如何生效，limits只有用户登陆的情况下执行命令才能生效，是通过pam加载生效的。需要加载pam模块。在/etc/pam.d/system-auth 里面查看加载的模块，以登录方式加载了哪些模块，

  limits 不生效的问题

* stderr stdout

  stderr=subprocess.STDOUT,stdout=subprocess.PIPE 

  表示stdout输出到管道中，stderr输出到Stdout中，这样可以从stdout中拿到错误和正确的输出

  rest.stdout.read()

  rest.communicate() 会把错误输出和正确输出返回到一个元组中。第一个标准输出，第二个标准错误

一般的使用方法

```python
res=subprocess.Popen(["/bin/bash","-c","-l","dir"],shell=True,stderr=subprocess.STDOUT,stdout=subprocess.PIPE)
res.wait() #等待程序命令执行完成后退出，如果直接退出会造成僵尸进程
# res.wait(5) #等待指定的时候后退出，如果超过指定的时间还没有执行完成会把命令kill掉
res.stdout.read()
# res.communicate() 
```

任何的架构命令执行这块是不变的。

#### 通信协议

agent和master直接通信，通过json数据进行交互，agent如何返回数据，

**agent和master直接如何建立连接**

* 单条连接tcp，发送任务和接收agent执行的结构数据

  优点：只需要建立一条连接，可以同时操作6万多个机器

  缺点：有状态，如一个连接发送过来，agent还没有执行master就挂掉，当agent执行完成后结果返回不去了。master永远不知道这个命令的执行结果了。

* 2条连接，一个接收请求，一个返回请求。

  优点：两个连接可以解决单个连接master挂掉不能得到结果的情况

  缺点：并发比单挑连接少

**命令的执行结果一定是agent主动push到master比较好。**

**有新的任务的时候，master和agent 如何通信，是push还是pull**

* pull

  agent轮询去主动获取任务，

* push

  master主动推任务到agent，agent本地需要维护一个任务队列

**agent并行执行任务还是串行执行任务，任务间的并行还是串行**

* 并行执行

  任务之间会有干扰，任务无法做编排。

* 串行执行

  一个任务执行完成，才会执行下一个任务。如发布任务，备份--停止服务--copy程序--启动程序，

  如果任务是并行的，就乱了。

**agent主动pull的方式，就需要和master保持长连接，因为不知道什么时候master会分配任务给agent，**

* 如何保持长连接：心跳和重连，需要一直保持连接，单连接断开后需要有一个中机制重新连接。

**agent挂掉怎么处理**

* agent发起这个任务后会一直保持任务直到获取结果超时，标记为失败
* agent挂掉之后，可以重新开始从master获取任务，而后执行。

**master如何保持任务队列**

* 交给靠谱的第三方(可以是数据库),进行保存任务。
*  单个agent，agent获取，master从数据库取任务，而后返回给agent

**任务队列，调度系统**

global_task_queue 这个平台一个，全局任务队列

local_task_queue 每个agent一个，

1、用户提交任务到
2、push到全局任务队列，
3、根据任务并行度，把任务服务之到合适的agent任务队列中
4、agent询问任务的时候只需看自己的任务队列是否有任务。

**引入scheduler**

主要就是任务调度，把全局任务中的任务分发给agent的队列。

**调度**

执行任务的时候其中一个主机失败了怎么办，其他的主机怎么办，是继续执行还是退。当发布任务的时候其中一台发布失败了怎么办，是继续发布时标记任务为失败，这时候就需要调度了。

允许一个任务多少台agent同时执行，控制任务的并行，允许多少台同时执行

失败率，失败率达到多少的时候时候标记任务为失败。

任务的超时

**scheduler的工作流程**

开始用户输入--->加入全局队列----->通知Scheduler调度器---->选择Target(根据并行度和target列表选择tatget)--->复制任务到agent队列--->target执行任务----->上报结果---->进行下一批调度任务(每一个agent执行完成，结果上报的时候都会执行调度)

调度的时候需要知道的一些数值
	target总数，并行度，失败数，成功数，正在执行的数量，最大允许的失败率

​	if 失败数> 失败率，则这个任务失败，退出

​	if 正在运行的数量<并行度，则执行调度。

​	if 失败数+成功数 == 总数 则执行成功，退出

**失败任务的重复执行**

​	这个有用户绝对，如果任务失败时都在失败的机器上进行重复执行任务。

**跨机房最不稳定的就是网络**

**任务流程**

用户提及任务到proxy,proxy把任务扔给全局队列，scheduler进行任务调度，根据并行度，分配给指定的agent队列，agent询问任务请求到master，master查询agent的任务度列，把任务返回给agent。agent执行任务把结果返回master.

![](..\image\image-20191222213922156.png)

**传递的任务数据**

```josn
{
	"command":" ",
	"timeout":180,
	"targets":[],
	"parallel":1, // 并发，1表示并行
	"failrate":0 // 失败率，0，表示有失败就退出
	"callback_url":""
}
```

**agent返回数据**

```jso
{
	"code":0
	"output":"标准错误和标准输出"
}
```

**队列服务**

队列服务的需求

* 持久化
* 高可用
* 全局队列时可修改，

Rabbitmq 支持持久化和高可用，但是不支持全局队列的修改，因为消息只能消费一次，不可修改。

**zookeeper 分布式协调服务**

节点   目录/目录

​	可以对节点进行读写删除操作，遍历节点内容

临时节点，连接断开节点自动删除。

watch  inotify 当节点有修改的时候可以通知watcher的客户端。

高可用，强一致性的，数据的修改，要么成功，要么失败，不会有第三种状态，修改数据后所有节点生效或者不生效。

强一致性，最终一致性  弱一致性

​	强一致性：在所有节点上要么成功，要么失败，同时可见

​	弱一致性：不保证所有节点都写入成功，

​	最终一致性，在一个节点上写入成功，最终会被通知所有节点，并不是所有节点立即可见。

DNS SRC记录 进行服务器发现

**设置zk的节点结果满足我们的系统要求**

```shell
msched
	|_tasks 全局任务队列
		|_$task_id task内容
			|_targets 对应这我们的所有agent id
				|_targets_id 每个agentid
					|_ 本次任务状态，当前agent的主席状态，
            |-lock 临时锁节点，进程退出后，则这个lock会删除
	|_agents 本地队列
		|_$agent_id  每个agent一个名称 
			|_heartbeat 心跳信息
			|_tasks agent任务队列
	|_signal 专门的通知节点
		|_$task_id
	|_callback
		|_$task-id
```

**通知机制：**

* 当tasks中有新的任务ID的时候需要通知scheduler工作，scheduler watch tash节点，任何一个task创建成功后都可以获取到，接收到通知，通过监听tasks的节点实现
* 任何一个agent执行完成自己的任务，上报结果后都会通知调度器，通过监控signal节点实现

zk的通知和watch机制是不递归，不能监控目录中的目录

**signal**

当我们的task中的agent节点的状态有变化的时候，在signal中set一个当前taskid的值。而后Scheduler开始监听agentID的节点，当要通知Scheduler的时候只要对signal中的tash id的值发生变化即可，这时候Scheduler会接收taskId，

**多个scheduler进程存在**

多个scheduler进程可能同时操作同一个节点，所以可以加锁， 当作task任务进行调度的时候可以在agent节点下创建lock节点。调度完成后lock节点删除。

**agent不用加锁是因为,agent是唯一的，每次agent请求到达master之后，只会有一个master会去修改agent的值**

**任务的删除**

当任务执行成功后就会删除任务。

**task中agent的状态**

* 失败
* 成功 
* 运行 
* 等待(task已经分配到agent队列，但是agent还没有执行),
* New(任务刚刚建立时候的状态)，
* kill(任务被中断时候的状态)

**执行过程**

当用户输入把任务加入全局队列，需要写两个节点，一个是task节点，一个是signal中的task节点，而后zk通知scheduler，进行调度，scheduler根据并发度从task中targets中根据任务并发度选择两个targets_id。而后把他的状态设置为wait,而后把task的内容复制到agent中的agent_id中的task里面。

master关注agent节点，某个agent上来询问是否有任务的时候， 会更新heartbeat节点， 而后检查task节点中是否有任务如果有返回给agent，把task中agent_id中任务状态设置为run状态。agent拿到任务去处理

agent执行完成后把结果返回给master，master做两件事情，把agent中agent_id对应的task删除掉，而后设置task中task_id中agent_id的状态，成功或者失败

**zk受限于连接数，3节点zk,每个节点30000连接，也就是100000，zk的节点数越多性能越差，一般是1、3、5，奇数个节点，**

**heartbeat**

消跳信息里面可以带一个基础的监控信息，负载，当前执行的任务，当前时间，agent版本，。。。

**agent的节点删除**

是由人工去删除的

**task任务节点的删除**

任务执行结束的时候删除，由scheduler决定任务是否结束。

**如何通知用户任务是否完成**

任务应该是通知完用户后删除task。所以需要一个callback模块进行通知

**用户如何实时接收到结果输出**

agent执行完成任务后，看到agent的输出，一种是agent执行的时候，实时看到输出。

* 任务执行完成后看到输出

  agent返回数据的时候返回输出，只用把数据进行持久存储即可，可以直接存储在kv存储中

  一般不会把这种数据输出到db中，因为db数据有16m的限制。

#### master进程

主要是为了解耦agent，如后面需要切成etcd则就不用修改agent端的代码，解耦，如果不为接口实际agent是可以直接连接zk，

#### proxy api进程

proxy会进行查询任务信息db和从kv存储中查询输出展示给db,给用户展示输出的信息。

#### callback进程

主要是进行通知用户的操作的，任务完成的时候通知用户并且把任务信息发给用户。

主要是用来监控zk的callback节点，当任务完成的时候设置任务id到callback节点中。而后callback基根据任务ID读取tasks中的任务信息了。而后通知用户

还需要做一些清理行的工作，如删除agent中的 agentid中的任务列表，删除task中的task任务，还有就是callback中的id删除。

把任务信息持久化，持久到db，进行审计工作。

#### monitor进程

我们可以加一个monitor进程，去监控节点的heartbeat信息，如果长时间没有就进行告警。或者直接ssh连接过去进行重启进程

#### 对agent进行验证

可以是token，或者其他算法进行认证hmac

#### 添加DB,KV持久化存储

实际可以把db和kv存储内容存储到一块，因为db有16m存储限制所以分开了。

输出存储到kv存储中，task_id_agent_id:输出结果

![image-20191225173331813](..\image\最终.png)

#### 跨机房的设计改进

![image-20191226190657628](..\image\跨机房.png)

getway主要是接收用户请求，而后调用不同机房中的api组件，进行分发任务，当任务执行完成后callback 回调getway 进行任务持久化，等其他操作，增加了一个logger组件专门来调用该getway进行日志传输，而后getway收到请求后进行把日志保存在kv中。

白色的框内的组件是部署在一个机房的

getway和机房直接的网络是不可靠的，如何保证接口的幂等性。

api如何规避网络的不可靠性：

​	当getway接收到task之后，gatway带着task id调用api,api从zk中查找任务id存在不存在，如果不存在则添加task id到zk中，如果存在则返回成功，这样getway可以重复调用api，直到api返回成功。

callback调用geteway保证可靠性

​	和gateway调用api的方式相同

logger调用gateway保证可靠性

​	master把结果输出logger的时候，logger需要进行本地存储，而后调用gateway，当调用成功则删除本地存储。

agent代码实现

​	主要是实现发送心跳包。

​	struct 解决粘包问题

