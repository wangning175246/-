## CAP原理：

C:一致性：每次总是能读取到最近写入的数据或者失败

A:可用性：每次请求都能够读取到数据

P:分区域容错性：系统能够继续工作，不管任意个消息有与网络失败

> P是必须满足AC,可以选择性满足
>
> AP 和CP

#### 注册中心

注册中心一般可以做成AP系统，做到最终一致性，因为如果是CP类型的，需要选举等以下操作，会使服务获取方可能暂时获取不大数据，所以就可用而言，可以做成AP的。

![image-20200421093414879](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200421093414879.png)

##### etcd使用场景

- 服务注册

- 共享配置

- 分布式锁

- Leader选举,

## 服务发现和注册

服务发现和注册可以在服务端做，也可以在客户端做，

​	服务器端注册注册中，客户端访问LB,LB可以通过注册中心查找而后调用。

​	客户端做是通过RPC，从注册中心中获取服务列表，而后调用

![image-20200418194530549](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200418194530549.png)

#### 服务发现的的实现

一般情况下注册中心挂掉不会影响我们的正常的服务，所以一般会把信息从etcd中获取的信息缓存到本地，不会一直去etcd中去请求。

<img src="C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200426100427115.png" alt="image-20200426100427115" style="zoom: 67%;" />

## 负载均衡算法

- 随机算法

  随机生成一个随机数，nodes[随机数]

- 轮询

  nodes[有序数]

- 加权随机算法

  ip1 3    ip2   2   ip3  1

  ip3 ip3 ip3  ip2 ip2 ip3

- 加权随机算法2

  对所有的节点的权重求和，

  而后随机生成一个数

  遍历所有节点，使用随机生成的数减去每个节点的权重，当结果小于0的时候返回当前遍历的节点。

  ![image-20200426152917923](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200426152917923.png)



## 分布式一致性问题

> https://zhuanlan.zhihu.com/p/32052223

##### Raft协议

- 解决分布式系统数据一致性的问题
- 基于复制做的

##### Raft种节点的角色

- **Leader**：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
- **Follower**：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
- **Candidate**：Leader选举过程中的临时角色。

##### 工作机制

- Leader选举

  Leader选举之前我们的系统是不可用的.

  集群初始化的时候需要选举

  从节点收到Leader的心跳包后,把选举定时器清零,则就不会触发选举了,如果指定时间内没有收到心跳包则会触发选举

- 日志复制

  所有写入都是Leader写入,而后把数据复制到从节点执行,从节点执行完成后通知主节点,主节点返回客户端

- 安全性

## GRPC

grpc 是基于http2的

不兼容http2的客户端通过协商机制(Upgrade机制，ALPN机制)来兼容。

#### Upgrade机制

客户端主动发起

​	如果服务端支持HTTP2,返回101 switching protocol

​	如果服务端不支持http2,直接忽略返回http1.1的内容

版本标识：

​	h2c，标示运行在明文TCP之上的HTTP/2协议，大多数浏览器不支持

​	h2，标示使用了TLS的http/2,大多使用过的

#### ALPN机制(应用层协商机制)

在https密钥交换协议的基础上加了一个扩展。

- 客户端主动发起

  如果服务端支持http2，返回selected h2

  如果服务端不支持HTTP2，则返回selected http1.1

- 协商的时机

  在https交换加密密钥的过程中进行协议

  ALPN只应用于https的http2.0

#### 非对称加密

私钥不能公开，一般会把公钥公开。

一般是私钥加密内容公钥解密内容，或者公钥加密私钥解密

通过私钥和加密算法和推导出公钥，不能通过公钥推导出私钥

#### 环境配置

1. 需要安装protoc
2. 安装插件 go get -u github.com/golang/protobuf/protoc-gen-go
3. 安装grpc包google.golang.org/grp

#### GRPC metadata 

在grpc中grpc对http头部传递数据进行了封装。metadata单独抽象了一个包：

​	`google.golang.org/grpc/metadata`,其实就是封装了一个map `type MD map[string][]string`

**客户端代码**

```go
package main

import (
	pb "Microservice/grpc_example/hello"
	"context"
	"fmt"
	"google.golang.org/grpc"
	"google.golang.org/grpc/metadata"
	"log"
	"os"
)

const (
	address ="localhost:8888"
	defaultName="world"
)

func main()  {
	//WithInsecure 标识不安全的
	conn,err:=grpc.Dial(address,grpc.WithInsecure())
	if err!=nil{
		log.Fatal("did not connect :%v",err)
	}
	defer conn.Close()
	c:=pb.NewHelloServiceClient(conn)
	name:=defaultName
	if len(os.Args)>1{
		name=os.Args[1]
	}
	for i:=0;i<1;i++{
        // 创建ctx,传递数据
		ctx:=context.Background()
        // 后面可以跟多个key，value
        //metadata.AppendToOutgoingContext(ctx,key,value,key1,value2,....)
		ctx=metadata.AppendToOutgoingContext(ctx,"koala_trace_id","9999999999")
         // grpc会把ctx中数据设置到请求头中传递到客户端。
		fmt.Println("xxxxx")
		r,err:=c.SayHello(ctx,&pb.HelloRequest{Name:name})
		if err!=nil{
			log.Println("could not greet :%v",err)
			continue
		}
		log.Printf("Greeting:%s",r.Reply)
	}

}

```

**服务端代码**

```go
func (s *SayHelloController)Run(ctx context.Context,r *hello.HelloRequest)(resp *hello.HelloResponse, err error){
	//grpc会自动解析请求头中的数据到ctx,获取客户端传递过来的数据。
	md,ok:=metadata.FromIncomingContext(ctx)
	if ok{
		fmt.Println("client",md)
	}
    return &hello.HelloResponse{Reply: "你好"+r.Name,},nil
}
```

## 代码生成工具

没有代码生成的缺点

- 每个服务器需要写一个共性的代码，

- 代码风格不一样
- 缺乏抽象
- 不支持自定义中间件
- 代码维护性差
- 用户定制化差

代码生成可以解决哪些事情

- 把框架功能和业务功能分离
- 代码质量高
- 服务可维护性高

服务目录规范

- controller：存放服务的方法实现
- idl：存放本服务的idl定义
- main：存放服务的入口代码
- scripts：存放服务的脚本
- conf：存放服务的配置文件
- app/router：存放服务的路由
- app/config：存放服务的配置文件
- model：存放服务的实体代码
- generate：grpc生成的代码
- router

#### 实现思路

- 创建需要的目录
- 解析proto文件
- 通过定义代码模板和解析的proto的信息生成相应的代码
  - router 主要是对路由和controller业务代码进行绑定，参数的校验，中间件代码的处理
  - generate 根据的定义的proto生成的grpc的代码
  - main函数的代码。主要是启动RPC服务
  - controller 主要存放的是业务处理的代码

##### controller

这个里面主要是存放我们的业务代码，也就是proto文件中的定义的rpc service.

添加router，并向grpc进行路由注册，后面请求会先经过route,router把请求分发到controller

每一个业务代码都有一个route,所有请求的限速，限流等都是在route中进行的

## 中间件

在处理业务前后的需要执行的一些操作，这部分功能我们叫做中间件

把业务处理函数和中间抽象出来，这边可以把他们都放到一个链表里面，

```go
package middleware

import (
	"context"
)

type MiddlewareFun func(ctx context.Context, req interface{}) (resp interface{}, err error)
type Middleware func(MiddlewareFun) MiddlewareFun

var userMiddeware []Middleware

// 链接所有的中间件
func Chain(outer Middleware,others ...Middleware)	Middleware  {
	return func(next MiddlewareFun) MiddlewareFun {
		for i:=len(others)-1;i>=0;i--{
			next=others[i](next)
		}
		//chain :=Chain(outer,middleware1,middleware2)
		//chain(proc)(context.Background(),"test")

		//outer(others[0](others[1](proc)))(context.Background(),"test")
		// 所以先输出 outer start
		// 执行next 也就是 others[0](others[1](proc))(context.Background(),"test")  输出 1 start
		// 执行 others[1](proc)(context.Background(),"test")    输出 2 start
		// proc(context.Background(),"test")  输出 req process start!
		//  而后根据返回就是 输出 req process end!
		// 2 end
		// 1 end
		return outer(next)
	}
}

func Use(m ...Middleware)  {
	userMiddeware=append(userMiddeware,m...)
}

func BuildServerMiddleware(handler MiddlewareFun)(handlerChain MiddlewareFun)  {
	var mids []Middleware
	if len(userMiddeware)!=0{
		mids=append(mids,userMiddeware...)
	}
	if len(mids)>0{
		m:=Chain(mids[0],mids[1:]...)
		return m(handler)
	}
	return handler
}
```

#### 测试代码

```go
package middleware

import (
	"context"
	"fmt"
	"math/rand"
	"testing"
)

func TestMiddleware(t *testing.T)  {
	middleware1:= func(next MiddlewareFun)MiddlewareFun {
		return func(ctx context.Context, req interface{}) (resp interface{}, err error) {
			fmt.Println("middleware 1 start ")
			num:=rand.Intn(10)
			if num<10{
				err=fmt.Errorf("this is request is not allow")
				return
			}
			resp,err=next(ctx,req)
			if err!=nil{
				return
			}
			fmt.Println("miiddleware 1 end ")
			return
		}
	}
	middleware2:= func(next MiddlewareFun)MiddlewareFun {
		return func(ctx context.Context, req interface{}) (resp interface{}, err error) {
			fmt.Println("middleware 2 start ")
			resp,err=next(ctx,req)
			if err!=nil{
				return
			}
			fmt.Println("miiddleware 2 end ")
			return
		}
	}

	outer:= func(next MiddlewareFun) MiddlewareFun {
		return func(ctx context.Context, req interface{}) (resp interface{}, err error) {
			fmt.Println("middleware outer start ")
			resp,err=next(ctx,req)
			if err!=nil{
				return
			}
			fmt.Println("miiddleware outer end ")
			return
		}
	}
	// 处理请求的业务代码
	proc:= func(ctx context.Context,req interface{})(resp interface{},err error) {
		fmt.Println("req process start!")
		fmt.Println(req)
		fmt.Println("req process end ")
		return
	}
	// 生成调用链,outer先执行,而后1 而后2 而后是下面的proc的业务
	chain :=Chain(outer,middleware1,middleware2)
	// 执行调用
	resp,err:=chain(proc)(context.Background(),"test")
	if err!=nil{
		fmt.Println(err)
	}
	fmt.Println(resp)
}
```

## 监控

go 提供了和promtheus集成的一个包，我们只需要引入这个包，他会把go里面常用的一些指标暴露出去

go gc的次数，go 的gc的延迟，go 的协程数量

```go
package main

import (
	"flag"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"log"
	"net/http"
)

var addr=flag.String("listent-addr",":8080","the address to listen on for http request")

func main() {
	flag.Parse()
	// 我们可以自定义端口,也可以不自定义端口,但是需要保证/metrics 这个路径没有绑定具体的业务代码
	http.Handle("/metrics",promhttp.Handler())
	log.Fatal(http.ListenAndServe(*addr,nil))
}
```

有时候还需要一些业务上的埋点，每秒处理多少请求，每个请求的延迟，所以好需要加上业务自己的采样点

我们可以在框架中添加一个prometheus的中间

#### promtheus的数据指标类型

> https://fuckcloudnative.io/prometheus/2-concepts/metric_types.html

```go
package main

import (
	"flag"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"log"
	"math/rand"
	"net/http"
	"time"
)


var (
	// 这个是采样瞬时值,可以是任意值
	cpuTemp = prometheus.NewGauge(prometheus.GaugeOpts{
		Name:"cpu_temerature_celsius",
		Help:"Current Temperatue of the cpu",
	})
	// 这个是采样增长的的一个值,只能增长或者清零
	hdFailure=prometheus.NewCounterVec(prometheus.CounterOpts{
		// 这个是监控指标的名称
		Name:"hd_errors_total",
		Help:"Number of hard-disk errors",
	},
	// 标签,可以通过标签进行筛选
	[]string{"device","service"},
	)
)

func init()  {
	// 注册到采样库,这样prometheus会自动采样
	prometheus.MustRegister(cpuTemp)
	prometheus.MustRegister(hdFailure)
}

var addr=flag.String("listent-addr",":8080","the address to listen on for http request")

func main() {
	go func() {
		for {
			val :=rand.Float64()*100
			// 设置cpu_temerature_celsius监控项的值
			cpuTemp.Set(val)
			// 设置hdFailure 的值,而后设置Lables,每秒加1
			// lable 主要是用于过滤数据,和区分不用设备的数据
			hdFailure.With(prometheus.Labels{
				"device":"/dev/sda",
				"service":"hello word",
			}).Inc()
			// 每隔1s 生成一次更新一次采样的值
			time.Sleep(time.Second)
		}
	}()
	flag.Parse()
	// 我们可以自定义端口,也可以不自定义端口,但是需要保证/metrics 这个路径没有绑定具体的业务代码
	http.Handle("/metrics",promhttp.Handler())
	log.Fatal(http.ListenAndServe(*addr,nil))
}

```

#### 监控中间件

需要当前的服务信息进行封装，而后把服务器信息添加到监控项的lable中，如哪个服务，哪个方法

- 当前的服务名 
- 当前的请求方法
- 当前的环境
- 当前的服务的集群，服务ID,服务机房
- 当前请求的trace_ID
- 当前服务器的IP
- 客户端的IP

存储主要通过context进行存储

在路由的时候，方法入口进行初始化

监控的指标

- 监控请求的数量
- 监控请求的错误数
- 监控请求的耗时

## 配置文件

把一个公共的配置提取出来放到配置文件中，代码生成的生成配置，

Yaml文件格式

- 大小写敏感，只允许使用空格，
- 缩进的空格数目不重要，只要相同的层级的元素左侧对其即可

yaml支持的数据格式

- 对象

  animal: pets

  对象的json格式 {“key”:"value"}

- 数组

  每个元素之前对应中划线

  \- Cat

  \- Dog

  对应的json格式

  ["age","name"]

- 纯量，常量

  字符串，布尔值，整数，NUll ，时间。日期

yaml文件的解析库：gopkg.in/yaml.v2

#### 框架配置梳理

- 服务监控端口
- Prometheus监听端口
- 服务的名字
- 注册中心配置
- 日志级别配置

#### 服务部署规范

- bin 生成的二进制的文件的存放路径
- conf 配置文件路径
  - product 线上配置文件
  - test  测试环境配置文件
- script 脚本文件
- Logs  日志文件

## 限流中间

限制流量，保证系统在突发的流量的情况下，系统能够正常允许，

保护有限的资源，不会被突发的大流量冲击而崩溃

#### 限流思路

- 排队

  引用场景，秒杀抢购，用户点击抢购之后排队，直到抢到或者售罄为止

- 拒绝

  出秒杀之外的任何场景

#### 限流常用的算法

- 计数限流算法
- 漏桶限流算法
- 令牌桶限流算法

#### 计数器

在单位时间内进行计数，如果大于设定的值的最大值，则进行拒绝

如果过了单位时间则重新进行计数

```go
package main

import (
	"fmt"
	"sync/atomic"
	"time"
)

type CounterLimit struct {
	counter int64   // 计数器
	limit int64		// 指定时间窗口内允许的最大请求数
	intervalNao int64 // 指定的时间窗口,实际就是每多长时间进行一次统计,计数器清零一次.只是通知之前30s的
	unixNao int64   // unix 时间戳,单位为纳秒
}

func NewCounterLimit(interval time.Duration,limit int64) *CounterLimit {
	return &CounterLimit{
		counter:     0,
		limit:       limit,
		intervalNao: int64(interval),
		unixNao:     time.Now().UnixNano(),
	}
}

func (c *CounterLimit) Allow()bool  {
	// 获取当前的时间
	now:=time.Now().UnixNano()
	// 判断当前时间和计数器初始化时间,
	if now-c.unixNao>c.intervalNao{
		atomic.StoreInt64(&c.counter,0)
		atomic.StoreInt64(&c.unixNao,now)
		return true
	}
	atomic.AddInt64(&c.counter,1)
	return c.counter<c.limit
}
func main()  {
	limit :=NewCounterLimit(time.Second,100)
	for i:=0;i<1000;i++{
		allow :=limit.Allow()
		if allow{
			fmt.Printf("i=%d is allow \n",i)
		}else {
			fmt.Printf("i=%d is not allow \n",i)
		}
	}
}
```

实现比较简单，处理突发流量会出现毛刺现象，某一时刻处理的特别多，后面的时间

比如1s限流100个请求，而一次来了200个请求，前100ms内处理了100个请求，后面900s时间内没有请求处理，需要等到下一秒才可以处理

计数不准确，记录的是绝对是时间，而不是相对时间

#### 漏桶算法

桶的流出的速率是固定的，

平滑流量，整体流量平稳，流量整形

解决了计数器限流算法的毛刺问题，整体流量控制的比较平稳。

因为它假设的后端服务的处理速率是一定的，

但是无法应对突发流量的问题。因为入口数是由限制。

一个固定大小的桶，以固定速率流出，水桶满了，则进行溢出(拒绝)

![img](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\漏桶.jpg)

```go
package main

import (
	"fmt"
	"math"
	"time"
)

type BucketLimit struct {
	rate float64		// 漏桶中水的漏出速率
	bucketSize float64	// 漏桶最多能装的水的大小
	unixNao int64		// 当前请求的时间
	curWate float64		// 当前桶里面的水
}

func NewBucketLimit(bucketSzie int64,rate float64)*BucketLimit  {
	return &BucketLimit{
		bucketSize: float64(bucketSzie),
		rate: rate,
		unixNao:    time.Now().UnixNano(),
		curWate:    0,
	}
}

func (b *BucketLimit)reflesh()  {
	now :=time.Now().UnixNano()
	diffset :=float64(now-b.unixNao)/1000/1000/1000
	//fmt.Println(diffset)
	// 每次都会算出当前桶里面的水是多少,diffset*b.rate 主要是计算漏出量,
    // 如果是付出，则标识流出的速率比流出速率小
	// math函数,里面的哪个值大,就返回哪个值,如果请求超出了,就把当前桶的剩余容量设置为0
	b.curWate=math.Max(0,b.curWate-diffset*b.rate)
	//fmt.Println(b.curWate)
	b.unixNao=now
	return
}

func (b *BucketLimit)Allow() bool  {
	b.reflesh()

	if b.curWate <b.bucketSize{
		fmt.Println(b.curWate)
		b.curWate=b.curWate+1
		return true
	}
	return false
}

func main()  {
	// 限速1qps ,桶的大小10
	limit :=NewBucketLimit(10,1)
	for i:=0;i<20;i++{
		allow:=limit.Allow()
		if allow{
			fmt.Printf("i=%d is allow\n",i)
		}else {
			fmt.Printf("i=%d is not  allow\n",i)
			time.Sleep(1*time.Second)
		}
	}
}
```

#### 令牌桶算法

这个用的比较多，漏桶和计数器用的比较少

令牌桶算法能够在限制数据的平均传输速率的同时还允许某种程度的突发传输。

一个固定大小的桶，以固定速率放入token,如果可以请求可以拿到token则处理否则就拒绝。

如果桶里面的令牌的是100个，则可以10毫秒内把100令牌全部取完，但是后续还是可以以固定的速率向桶中存放令牌。可以应对突发流量，

![img](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\令牌.jpg)

```go
package main
// 限流算法测试
import (
	"context"
	"fmt"
	"golang.org/x/time/rate"
	"net/http"
	"time"
)
// WaitN 和Wait 会阻塞等待
func main_1() {
	// 令牌桶的容量是5,如果里面的不足5个.则每秒向桶里面放置1个
	r:=rate.NewLimiter(1,5)
	ctx:=context.Background()
	for {
		// 如果可以取出来则执行我们的业务代码. 每次从桶里面取1个.能取到则返回空,否则一直等待等到可以取到为止.
		// 效果不变，还是每秒输出一次
		//err:=r.Wait(ctx)
		// 表示每次取两个,
		// 前几个是每秒输出一次，后面是每2s输出一次。
		err:=r.WaitN(ctx,2)
		if err!=nil{
			fmt.Println(err)
		}
		fmt.Println(time.Now().Format("2020-01-02 15:04:05"))
		time.Sleep(time.Second)
	}
}
//AllowN 和Allow 如果限速了不会阻塞，而是会直接返回True
func main_2()  {
	r:=rate.NewLimiter(1,5)
	for {
		//r.AllowN(time.Now(),2) 如果取不到结果,不会阻塞而是会直接返回False,
		if (r.AllowN(time.Now(),2)){
			fmt.Println(time.Now().Format("2020-01-02 15:04:05"))
		}else {
			fmt.Println("限速了")
		}
		time.Sleep(time.Second)
	}
}
var  r=rate.NewLimiter(1,5)

func MyLimit(next http.Handler) http.Handler {
	return http.HandlerFunc(func(write http.ResponseWriter,request *http.Request){
		if !r.Allow() {
			http.Error(write,"访问次数过多",http.StatusTooManyRequests)
			return
		}else {
			next.ServeHTTP(write,request)
		}
	})
}

func main()  {
	mux:=http.NewServeMux()
	mux.HandleFunc("/", func(write http.ResponseWriter,request *http.Request) {
		//一般不会这样取些，一般会写一个包装器
		//if r.Allow(){
		//	write.Write([]byte("OK!!"))
		//}else {
		//	http.Error(write,"访问次数过多",http.StatusTooManyRequests)
		//}
		write.Write([]byte("OK!!"))
	})
	http.ListenAndServe(":8080",MyLimit(mux))
}
```

#### 分布式限流算法

需要使用redis,在redis中设置一个key，过期时间是1s，而后每来一个请求这个key的值加1，当大于某个值的时候拒绝请求。

#### 限流中间件

可以实现一个通过接口，因为有n中限流算法，所以我们可以使用中间件。

## 日志库

#### 日志分级

- Debug：主要是用于调试程序，日志最详细，开发测试环境使用
- Trace：主要用于追踪定位问题，查询问题时候使用
- Info：用来记录程序允许中的一些关键信息，
- Warn：当程序触发了异常分支，但系统可以恢复到正常状态
- Error：当前程序发生了严重级别错误，必须马上处理，否则系统无法运行

#### 分布式追踪的支持

- 对于同一个请求，生成一个TranceID，然后传递

  - 把这个请求经历的所有子系统的日志进行聚合查询

- TranceId 生成的时机

  - 在请求入口生成

- TranceId 生成的规则当前的时间+随机数

- TranceID 如何在进程内传递

  进程内通过context传递

  跨进程，通过grpc的header进行传递

#### 日志的聚合 Addfield的支持

把一写重要的信息，聚合成一条日志

避免日志太多，太繁琐

在调用access的时候，把当前所有的Addfield的字段进行输出

#### 实现

- 日志调用接口
- 日志输出器
  - file
  - stdout

这个两个通过chan进行交互。程序调用日志接口失败，不会影响程序的业务逻辑

####  日志库中间件

## 分布式追踪

#### trace_id

为每个请求分配一个唯一的ID,通常叫做trace_id，经历不同的系统都会带有这个trace_id。

日志聚合

#### Span概念

要追踪的一个环节，如何知道每个子系统的详细的处理细节。

通过span进行抽象。追踪的每一个环节就是一个span

Span 之际可以进行抽象，父Span可以多个子Span

![image-20200501150652800](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200501150652800.png)

![image-20200505105749662](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200505105749662.png)

#### 单个Span存储的信息

![image-20200501151427647](C:\Users\18434\OneDrive\笔记\go\项目\image\微服务\image-20200501151427647.png)

span name 这个span的名字

trace id 请求的ID

span id span的 ID

span parent id 这个span服务span的ID

Client

​	Start 开始

​	Send 客户端发送请求的时间

​	Recv 客户端接收到请求的时间

​	End 结束

Server 

​	Recv 客户端接收到请求的时间

​	"foo" 人为的给span设置的标签，这个span对应的操作是什么，可以是执行的当前函数的名称

​	Send 服务端返回请求的时间。

可以通过Server Recv - Server Send 可以知道服务端处理请求的时间

可以通过  Clinet Recv - Server Recv 可以知道这个请求在网络传输中消耗的时间。

#### Span Context其他用途

我们在context 中设置请求时来自灰度环境还是正常的环境的请求，来进行线上压测等。

#### 实现

进程内传播：通过context进行传播，设置Span的信息到context中，

进程间传播：http协议，通过http头部进行传播

Tcp：通过改造Thrift协议才可以支持

Jaeger、Zikin、Opentracing-go

 Opentracing 定义了一个分布式追踪协议的规范，Zikin和Jaeger都实现了这个规范

#### jaeger

span的上传端口 6831

UI页面的访问地址 16686

```go
package main

import (
	"fmt"
	"github.com/opentracing/opentracing-go"
	"github.com/opentracing/opentracing-go/log"
	"github.com/uber/jaeger-client-go"
	jaegercfg "github.com/uber/jaeger-client-go/config"
	"io"
	"time"
)
func Init(service string) (opentracing.Tracer, io.Closer) {

	// 初始化配置
	cfg := jaegercfg.Configuration{
		ServiceName: service,
		// 采样的配置
		Sampler:     &jaegercfg.SamplerConfig{
			// 记录采样的配置，可以是
			//	 const 表示实时采样
			// probabilistic 表示概率采样，根据配置Param的值进行采样，如是0.1表示，10条采样值中只是记录1条
			//  https://rocdu.gitbook.io/jaeger-doc-zh/architecture/sampling
			Type:  jaeger.SamplerTypeConst,
			// 表示是否有开启
			Param: 1,
		},
		// 记录上报的配置
		Reporter:    &jaegercfg.ReporterConfig{
			LogSpans: true,
			//LocalAgentHostPort指示记者将span发送到这个地址的jaeger-agent
			LocalAgentHostPort:"192.168.80.10:6831",
		},
	}
	tracer, closer, err := cfg.New(service,
		jaegercfg.Logger(jaeger.StdLogger),
	)
	if err != nil {
		panic(fmt.Sprintf("ERROR: cannot init Jaeger: %v\n", err))
	}
	return tracer, closer
}
func main()  {

	// 在jaeger中查看到服务名称
	tracer,closer:=Init("hello-word")

	defer closer.Close()
	
	helloTo:="wallace"

	// 开始一个叫say-hello的span
	span:=tracer.StartSpan("say-hello")

	// 该当前span设置的标签
	span.SetTag("hello-to",helloTo)
	time.Sleep(3*time.Second)
	
	helloStr:=fmt.Sprintf("hello %s!",helloTo)
	
	// 当前span中程序的输入的日志信息，
	span.LogFields(
		log.String("event","string-format"),
		log.String("value",helloStr),
	)

	println(helloStr)
	time.Sleep(3*time.Second)
	// 也可以通过这种方式写日志
	span.LogKV("event","println")
	
	// 结束say-hello的span
	span.Finish()
	
}
```

而后可以在web页面中查看上传的信息。

#### 框架添加分布式追踪代码

1.  需要添加配置，是否开启分布式追踪
2. 添加配置解析
3. 配置中间件

## RPC的客户端调用库的实现

#### 简单的实现

```go
package main

import (
	pb "Microservice/grpc_example/hello"
	"context"
	"google.golang.org/grpc"
	"log"
	"os"
)

const (
	address ="localhost:8888"
	defaultName="world"
)

func main()  {
	//WithInsecure 标识不安全的
	conn,err:=grpc.Dial(address,grpc.WithInsecure())
	if err!=nil{
		log.Fatal("did not connect :%v",err)
	}
	defer conn.Close()
    // 生成客户端调用实例
	c:=pb.NewHelloServiceClient(conn)
	name:=defaultName
	if len(os.Args)>1{
		name=os.Args[1]
	}
	for i:=0;i<1;i++{
		ctx:=context.Background()
        // 调用具体的函数，拿到结果
		r,err:=c.SayHello(ctx,&pb.HelloRequest{Name:name})
		if err!=nil{
			log.Println("could not greet :%v",err)
			continue
		}
		log.Printf("Greeting:%s",r.Reply)
	}

}
```

#### 最终给用户提供的调用接口

```go
package main

import (
	"context"
	"time"

	"github.com/ibinarytree/koala/logs"
	"github.com/ibinarytree/koala/rpc"
	"github.com/ibinarytree/koala/tools/koala/client_example/generate/client/helloc"
	"github.com/ibinarytree/koala/tools/koala/client_example/generate/hello"
)

func main() {
    // 创建客户端调用实例，并初始化一些元信息
	client := helloc.NewHelloClient("hello", 
		// 限流的配置
		rpc.WithLimitQPS(5),
        // 客户端的名称的
    	rpc.WithClientServiceName("hello-client-example"))
    ctx := context.Background()
    // 核心的代码都在这个里面
    resp, err := client.SayHello(ctx, &hello.HelloRequest{Name: "test my client"})
    fmt.printf(resp)
}
```

#### grpc客户端库设计

为了避免重复写客户端调用代码，所以客户端调用代码可以自动生成，

在生成的grpc的代码上扩展我们的客户端，通过中间件的方式实现我们需要的其他功能

#### 代码生成

我们也需要代码生成工具去生成基础的代码，

​	如：我们自动生成helloClient,而后用户通过helloClient去调用hello模块的功能，为各个模块生成各自的clinet,

#### PrepareMiddleware

主要是从请求头中获取TraceID。如果有表示是其他地方调用过来的，如果没有则表示是自己发起的请求，而后创建TraceID

#### 日志 中间件

在发起请求之前记录日志

#### 链路追踪 中间件

#### 监控 中间件

#### 限流中间件

#### 熔断中间件

主要是在调用方在客户端，当服务器的相应错误率高于某个的值的时候对部分请求进行熔断。

服务器请求失败的原因

- 网络原因

  - 网络连接建立慢或者失败
  - 网络请求超时
  - 服务过载
  - 网络抖动

- 服务部分过载

  - 部分机器挂掉
  - 流量突增，资源不足
  - 部分网络挂掉

- 策略 重试还是会造成出现错误，服务调用链比较多的情况，重试会造成流量放大

  一般情况下重试是在最外层(nginx、客户端)进行重试，

  或者部分非常重要的服务内部可以进行重试1次，不建议重试多次

##### 重试策略的问题

因为服务调用关系复杂

- 并发请求阻塞，关键资源直接到超时才释放

  - 内存消耗
  - 线程消耗
  - 数据库连接消耗

- 现象

  - 内存耗尽
  - 线程被占关
  - 数据库连接被占光，请求被hang主

- 后果

  服务雪崩

##### 解决方案

尽早解决，方向后端服务承受不住了，直接不把发请求发送到后端，直接报错返回给上层调用

##### 熔断机制

阻止有潜在失败可能性的请求，如果一个请求有较大的失败的可能，那么就应该及时拒绝这个请求，

对一个发送请求的成功率进行预测(采用机器学习，大数据统计)

实现思路：

​	针对每一个请求的结果，比如失败或者成功进行统计

​	在一定时间窗口内，如果失败率超过一个比率，那么熔断器直接打开

​	过一段时间熔断器在打开，这个打开会存在一定的问题，因为如果后端的服务还没有正常，熔断器打开后，还熔断，所以引入了半打开状态，在半打开状态，只有非常有有限的请求会正常进行，这些请求任何一个失败，都会再次进入熔断器打开状态，如果这些请求全部成功，则熔断器关闭。

##### 熔断器的状态

- 关闭状态，

  所有请求可以正常访问

- 开启状态，

  拒绝所有事情去

- 半开状态：

  开启状态的时候，每隔一段时间会进入一次半开状态，进入半开状态后每秒会发送一个请求到后端，

  可以允许部分请求，这部分请求的结果都正常后，则熔断器关闭，有一个不正常都重新开启

##### Hystrix

- 过载保护

- 熔断器，快速失败，快速恢复

  如果服务处于过载的请求，应该让服务快速失败，减轻后端服务处理的一个风险

- 并发控制，防止单个依赖把线程全部消耗完成

- 超时控制，防止永远阻塞

配置

​	Timeout 超时配置，默认100ms

​	MaxConcurrentRequest	并发控制，默认每个后端的并发是10

​	SleepWindow	熔断器打开后，冷却时间，默认是500ms

​	RequestVolumeThreshold 	一个统计窗口的请求数量，默认是20

​	ErrorPercentThreshold	失败率百分比 默认是50%

触发条件

​	一个统计窗口内，请求数量大于RequestVolumeThreshold 且失败率大于ErrorPercentThreshold才会被触发

#### 服务发现中间件

通过提供的调用的服务名称，从etcd中获取要调用的服务列表，

#### 负载均衡中间件

根据服务发现组件获取到的服务IP，而后选择具体的IP，

后续需要判断这个请求是否正常，如果不正常，需要进行重试

#### 建立连接的中间件

根据选择Ip，获取连接，没有则新建立连接

#### 具体的业务中间件

实现具体的业务代码。

