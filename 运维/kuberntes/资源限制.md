#### kubernetes资源类型

资源可以分为可压缩资源和不可压缩资源，

可压缩资源包含：CPU

不可压缩资源包含：内存 存储

CPU的资源的分配方式 

* 基于CPU亲和性
* 基于CPU时间片

#### 系统资源限制

```
      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available for pods)  |
|                         |
|                         |
---------------------------
```

allocatable 表示pod可以使用的所有资源,如果pod

kube-reserved  表示为kubernetes系统进程争取的预留资源，

eviction-threshold 驱逐的内存限制，

system-reserved    系统预留的内存资源

如下示例

```txt
节点有32Gi个memory，16 CPUs和100Gi个Storage
--kube-reserved 被设置为 cpu=1,memory=2Gi,ephemeral-storage=1Gi
--system-reserved 被设置为 cpu=500m,memory=1Gi,ephemeral-storage=1Gi
--eviction-hard 被设置为 memory.available<500Mi,nodefs.available<10%
Allocatable`将是14.5 CPUs，28.5Gi具有内存和 88Gi本地存储空间 
```

#### pod资源的限制

* request

  容器使用的最小资源，作为容器调度时资源分配的依赖，只有当节点上可分配资源量>=容器资源请求时才会调度到该节点

* limit

  容器能够使用的最大资源量，

#### 使用资源超出限制

如果服务器的资源足够，但是容器的实际的资源使用量超过了limit限制，如果容器的重启策略设置的可重启，则会重启该容器

#### 节点资源不足的处理策略

1. 进行资源回收
2. pod驱逐

#### 资源回收

会删除死掉的pod或者容器，删除无用的镜像

#### pod的驱逐条件

如果node的缩资源不足，分为一下两种情况

* 可压缩资源的不足，不会导致pod驱逐，系统会重新分片权重来限制Pod的cpu使用。

* 不可压缩资源的不足，会发生pod的驱逐，保证该节点上有足够的资源可用。

#### 驱逐策略

可以定义阈值告诉kubelet在什么情况下驱逐pod，

* 软驱逐

  达到阈值后，不会立即驱逐，而是等待一个用户配置的宽限期后驱逐，如果pod设置有 TerminationGracePeriodSeconds 则回取其中的较小值，

* 硬驱逐

  立即kill pod

  默认的硬驱逐策略

  ```shell
  memory.available<100Mi
  nodefs.available<10%
  nodefs.inodesFree<5%
  imagefs.available<15%
  ```

每次驱逐只会驱逐一个pod，每次计算要驱逐的pod的事件间隔默认时10s

如果节点的资源负载，已经达到了硬驱逐阈值或者软驱逐阈值，则节点会报告自己处于压力之下，则调度器就不会在向该节点调度其他pod。

## Pod 资源限制

requests和limits

k8s支持管理pod的cpu和memory两种计算资源，每种资源可以通过spec.container[].resources.requests和spec.container[].resources.limits两个参数管理

默认值

requests未设置时，默认与limits相同。

limits未设置时，默认值与集群配置相关。

- requests 请求值

  requests用于schedule阶段，k8s scheduler在调度pod时会保证所有pod的requests总和小于node能提供的计算能力。

  requests.cpu会被转换成docker run的–cpu-shares参数。该参数与cgroup cpu.shares功能相同，具体为： 

  用来设置容器的cpu使用的相对权重。

  该参数只有在CPU资源不足时生效，系统会根据容器requests.cpu的比例来分配cpu资源给该容器。例如两个container设置了相同的requests.cpu，当两者抢占CPU资源时，系统会给两个container分配相同比例的CPU资源。

  CPU资源充足时，requests.cpu不会限制单个container占用的最大值，即单个container可以独占CPU。

  requests.memory没有对应的docker run参数，只作为k8s调度依据。

  可以使用requests来设置各容器需要的最小资源，使得k8s可以将更多的pod分配到一个node来实现超卖。

- limits 限制值

  limits用于限制运行时容器占用的资源。

  limits.cpu会被转换成docker run的–cpu-quota参数。该参数与cgroup cpu.cfs_quota_us功能相同，具体为： 

  用来限制容器的最大CPU使用率。

  cpu.cfs_quota_us参数与cpu.cfs_period_us通常结合使用，后者用来设置时间周期，前者设置在时间周期内可以使用的CPU时间。两者的比例即为CPU的最大使用率。

  k8s将docker run的–cpu-period参数设置为100000，即100毫秒。该参数就对应着cgroup的cpu.cfs_period_us参数。

  limits.cpu的单位使用m，意义为millicore，即千分之一核。250m就表示25%的最大cpu使用率，k8s会将其转换为25毫秒的-cpu-quota参数。

  limits.memory会被转换成docker run的–memory参数。用来限制容器使用的最大内存。

  当容器申请内存超过limits时会被终止，并根据重启策略进行重启。

  容器的CPU使用率不允许长时间超过limits，当超过limits时不会被终止。